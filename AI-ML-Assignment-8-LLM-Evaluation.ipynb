{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072793de-9467-46f4-b2a5-10d72d619161",
   "metadata": {},
   "source": [
    "## Step 1: Import Set-up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f87f08e-3399-4f20-8547-f17293ddba1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Pre-Check ===\n",
      "Torch version:          2.9.1+cpu\n",
      "Transformers version:   4.57.3\n",
      "PEFT version:           0.18.0\n",
      "Datasets version:       4.4.1\n",
      "Scikit-learn version:   1.7.2\n",
      "NumPy version:          2.3.5\n",
      "Evaluate version: 0.4.6\n",
      "\n",
      "=== Device Check ===\n",
      "CUDA available: False\n",
      "Device in use:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Device Check\n",
    "import torch\n",
    "import transformers\n",
    "import peft\n",
    "import datasets\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "print(\"=== Environment Pre-Check ===\")\n",
    "print(f\"Torch version:          {torch.__version__}\")\n",
    "print(f\"Transformers version:   {transformers.__version__}\")\n",
    "print(f\"PEFT version:           {peft.__version__}\")\n",
    "print(f\"Datasets version:       {datasets.__version__}\")\n",
    "print(f\"Scikit-learn version:   {sklearn.__version__}\")\n",
    "print(f\"NumPy version:          {np.__version__}\")\n",
    "print(f\"Evaluate version: {evaluate.__version__}\")\n",
    "\n",
    "print(\"\\n=== Device Check ===\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device in use:  {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafc3368-068c-4765-91c2-2e4da2d5bccc",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ec62681-442b-45d5-accd-8904694ba90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load IMDb dataset\n",
    "raw = load_dataset(\"imdb\")\n",
    "\n",
    "# Create validation split from train (stratified)\n",
    "splits = raw[\"train\"].train_test_split(test_size=0.2, stratify_by_column=\"label\", seed=42)\n",
    "train_ds = splits[\"train\"]\n",
    "val_ds = splits[\"test\"]\n",
    "\n",
    "# Use the original IMDb test set (already balanced)\n",
    "test_ds = raw[\"test\"]\n",
    "\n",
    "dataset = DatasetDict({\"train\": train_ds, \"validation\": val_ds, \"test\": test_ds})\n",
    "\n",
    "# ⚡ For CPU debugging, shrink dataset but shuffle first to keep balance\n",
    "dataset_small = DatasetDict({\n",
    "    \"train\": dataset[\"train\"].shuffle(seed=42).select(range(500)),\n",
    "    \"validation\": dataset[\"validation\"].shuffle(seed=42).select(range(200)),\n",
    "    \"test\": dataset[\"test\"].shuffle(seed=42).select(range(100))\n",
    "})\n",
    "dataset = dataset_small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19371f61-3f10-4a14-a628-4673227d43af",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization & Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff9fff52-fbb8-4fac-9265-5e15314560f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f5bdb13f924aba899aac4fcc7ea849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, legacy=False)\n",
    "\n",
    "def preprocess_fn(examples):\n",
    "    inputs = [f\"review: {t}\" for t in examples[\"text\"]]\n",
    "    labels_text = [\"negative\" if l == 0 else \"positive\" for l in examples[\"label\"]]\n",
    "    enc = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\")\n",
    "    enc_targets = tokenizer(text_target=labels_text, max_length=5, truncation=True, padding=\"max_length\")\n",
    "    enc[\"labels\"] = enc_targets[\"input_ids\"]\n",
    "    return enc\n",
    "\n",
    "tokenized = dataset.map(preprocess_fn, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9363fb0-331b-48a1-b64b-fdfb4873a983",
   "metadata": {},
   "source": [
    "## Step 4: Baseline Comparison (No Fine‑Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14219d3c-dce3-45f0-86d5-55886b358d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline (no fine-tuning) on 200 validation samples:\n",
      "Accuracy: 0.485\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marky\\anaconda3\\envs\\llm-finetune\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "from transformers import pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "baseline_pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=baseline_model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "def baseline_predict(texts):\n",
    "    prompts = [f\"review: {t}\" for t in texts]\n",
    "    # Tokenize with truncation to avoid >512 tokens\n",
    "    enc = tokenizer(prompts, max_length=256, truncation=True, return_tensors=\"pt\", padding=True)\n",
    "    outs = baseline_model.generate(\n",
    "        input_ids=enc[\"input_ids\"],\n",
    "        attention_mask=enc[\"attention_mask\"],\n",
    "        max_new_tokens=3\n",
    "    )\n",
    "    preds_str = tokenizer.batch_decode(outs, skip_special_tokens=True)\n",
    "    return [1 if \"positive\" in s.lower() else 0 for s in preds_str]\n",
    "\n",
    "sample = dataset[\"validation\"].select(range(200))\n",
    "baseline_preds = baseline_predict(sample[\"text\"])\n",
    "baseline_refs = sample[\"label\"]\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "print(\"\\nBaseline (no fine-tuning) on 200 validation samples:\")\n",
    "print(\"Accuracy:\", accuracy.compute(predictions=baseline_preds, references=baseline_refs)[\"accuracy\"])\n",
    "print(\"Precision:\", precision.compute(predictions=baseline_preds, references=baseline_refs, average=\"binary\")[\"precision\"])\n",
    "print(\"Recall:\", recall.compute(predictions=baseline_preds, references=baseline_refs, average=\"binary\")[\"recall\"])\n",
    "print(\"F1:\", f1.compute(predictions=baseline_preds, references=baseline_refs, average=\"binary\")[\"f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a606742-c6fd-461d-9982-14e0095c44d7",
   "metadata": {},
   "source": [
    "## Step 5: PEFT + LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f835258-76cd-4e28-838f-06e38fdf7328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 60,801,536 || trainable%: 0.4850\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2052487-9a36-490c-89a5-938eafb32a2a",
   "metadata": {},
   "source": [
    "## Step 6: Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5f85230-17ab-49a8-ab16-4924281a7c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marky\\AppData\\Local\\Temp\\ipykernel_12764\\4087586163.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 02:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.092200</td>\n",
       "      <td>0.085823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: ec609d31-e0f9-4974-a763-babc1c998cad)')' thrown while requesting HEAD https://huggingface.co/t5-small/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "C:\\Users\\marky\\anaconda3\\envs\\llm-finetune\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.09115130043029786, metrics={'train_runtime': 171.6717, 'train_samples_per_second': 2.913, 'train_steps_per_second': 0.728, 'total_flos': 17030971392000.0, 'train_loss': 0.09115130043029786, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bfefb1-80c4-4dca-bcd6-9b372ce7ffc3",
   "metadata": {},
   "source": [
    "## Step 7: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38947687-2675-4cb3-a1d0-6d37bc517c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics:\n",
      "Accuracy: 0.8\n",
      "Precision (Macro): 0.8044646548160397\n",
      "Recall (Macro): 0.795664391810518\n",
      "F1-Score (Macro): 0.797077922077922\n",
      "\n",
      "Classification report (test):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.78      0.87      0.82        53\n",
      "    positive       0.83      0.72      0.77        47\n",
      "\n",
      "    accuracy                           0.80       100\n",
      "   macro avg       0.80      0.80      0.80       100\n",
      "weighted avg       0.80      0.80      0.80       100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import evaluate\n",
    "\n",
    "# Load metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def to_int_labels(strs):\n",
    "    return [1 if \"positive\" in s.lower() else 0 for s in strs]\n",
    "\n",
    "# Test evaluation (Assignment 7 held-out test set)\n",
    "test_out = trainer.predict(tokenized[\"test\"])\n",
    "test_preds_str = tokenizer.batch_decode(test_out.predictions, skip_special_tokens=True)\n",
    "test_labels_str = tokenizer.batch_decode(test_out.label_ids, skip_special_tokens=True)\n",
    "\n",
    "test_preds = to_int_labels(test_preds_str)\n",
    "test_refs = to_int_labels(test_labels_str)\n",
    "\n",
    "print(\"Test metrics:\")\n",
    "print(\"Accuracy:\", accuracy.compute(predictions=test_preds, references=test_refs)[\"accuracy\"])\n",
    "print(\"Precision (Macro):\", precision.compute(predictions=test_preds, references=test_refs, average=\"macro\")[\"precision\"])\n",
    "print(\"Recall (Macro):\", recall.compute(predictions=test_preds, references=test_refs, average=\"macro\")[\"recall\"])\n",
    "print(\"F1-Score (Macro):\", f1.compute(predictions=test_preds, references=test_refs, average=\"macro\")[\"f1\"])\n",
    "\n",
    "print(\"\\nClassification report (test):\")\n",
    "print(classification_report(test_refs, test_preds, target_names=[\"negative\", \"positive\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd0e8e8-caa3-4bef-925e-48b93e02a127",
   "metadata": {},
   "source": [
    "## Step 8: Inference on Custom Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "979ed645-8840-4136-a72c-b2c2a5875add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: positive\n",
      "Test 2: negative\n",
      "Test 3: negative\n"
     ]
    }
   ],
   "source": [
    "def classify_review(text: str):\n",
    "    prompt = f\"review: {text}\"\n",
    "    gen = trainer.model.generate(**tokenizer(prompt, return_tensors=\"pt\"), max_new_tokens=3)\n",
    "    pred_str = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "    return \"positive\" if \"positive\" in pred_str.lower() else \"negative\"\n",
    "\n",
    "# Clear positive case\n",
    "print(\"Test 1:\", classify_review(\"This movie was amazing!\"))  # Expected: positive\n",
    "\n",
    "# Clear negative case\n",
    "print(\"Test 2:\", classify_review(\"Terrible acting and a boring plot.\"))  # Expected: negative\n",
    "\n",
    "# Ambiguous/mixed sentiment case\n",
    "print(\"Test 3:\", classify_review(\"The visuals were stunning, but the story was weak.\"))  # Model’s prediction may vary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2179af0e-4887-422d-b3de-8545d6394c01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-finetune)",
   "language": "python",
   "name": "llm-finetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
